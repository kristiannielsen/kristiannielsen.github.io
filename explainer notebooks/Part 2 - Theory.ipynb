{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 Theory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# imports\n",
    "import json\n",
    "import pandas as pd\n",
    "from sklearn.cluster import k_means\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# load the data\n",
    "df = pd.read_csv(\"new_clean_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section will explain and show how K-means clustering is performed on the data set. From the introductory analysis seen in part 1, it became evident that the distribution of all factors are not directly correlated to total amount of incidents happening. Therefore, it could be interesting to locate the most central points where these other factors are peaking. This kind  of knowledge could be utilized to target specfic types of incidents in the places where it would the greatest effect. And the same methodology could be applied on many different kinds of factors and/or vehicle types that cannot easily be identified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#filter the data\n",
    "slippery_related = df[df[\"contributing factor vehicle 1\"] == \"Pavement Slippery\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#picking the location columns\n",
    "X = slippery_related[[\"lon\",\"lat\"]]\n",
    "\n",
    "# list for holding the labels for the different ks\n",
    "all_data = X.copy()\n",
    "all_labels = []\n",
    "centroids_df = pd.DataFrame()\n",
    "all_centroids = {}\n",
    "all_inertia = []\n",
    "\n",
    "# run kmeans 5 times with k = 2..6\n",
    "for k in xrange(2,7):\n",
    "    centroids, labels, inertia = k_means(X, n_clusters=k) # inertia is the sum of squared errors for the model\n",
    "    all_data[\"k{}\".format(k)] = labels\n",
    "    all_labels.append(labels)\n",
    "    all_inertia.append(inertia)\n",
    "    all_centroids[\"k{}\".format(k)] = [(x,y) for x,y in centroids]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The method known as the *Elbow Method* is used to decide the best number of clusters. Below is the inertia plottied against the value of `k`. This value is a measure of the closest distance a point has to one of the `k`-centers. The place where the line flattens the most is a good basis for a value of `k`. In this, that would be 4 or 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYMAAAEPCAYAAACgFqixAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1136994d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.tight_layout\n",
    "plt.ylabel(\"Inertia (SSE)\")\n",
    "plt.xlabel(\"K-value\")\n",
    "plt.plot(np.arange(2,7,1), all_inertia)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save the geodata as csv\n",
    "all_data.to_csv(\"slippery_geodata_with_ks.csv\", index=False)\n",
    "\n",
    "# and the centroids as json\n",
    "with open('slippery_centroids.json', 'w') as file:\n",
    "    json.dump(all_centroids, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section two different kinds of regression will be performed, RandomForestRegressor and MLPRegressor. RandomForestRegressor is a decision tree based regression algorithm used to estimate continouos values. The MLPRegressos is a multi layer perceptron (more commonly known as a neural network) based regressor. Two different models are used to form a basis for comparison.\n",
    "\n",
    "In preparation for the analyses, the data is filtered and processed to the right format. First, several features of the data set were chosen. For both analyses `[\"borough\", \"year\", \"month\", \"weekday\", \"hour\"]` which all can be considered as categorical features. Second, the data was grouped to get an hourly count of the accidents in each borough for every date (both year, month, and weekday) in data set. Third, the features were converted using pandas `get_dummies()` that converts them into numerical indicator values. As oppposed to e.g. one hot encoding this ensures no hierachy among the values of the different features and instead new boolean dummy variables are created.\n",
    "\n",
    "Before applying the regressors, the data was split into a training and a test part to avoid overfitting the data. Additionally, the `SelectFromModel` model from the `sklearn.feature_selection` library was applied to `RandomForesRegressor` to see if any improvements could be achieved. This model selects the most important features based on the a model's `feature_importances_` that tells how much a feature in the input says about the predicted output.\n",
    "\n",
    "To measure the performance of the regressors, crossvalidation was utilized to gain a more robust measure of the performance of each regressor. The performance for each of the regressors were measured by $R^2$ number. Different versions of this measurer exist, but in `scikit`-version, which is used for this task, it is defined as $1-\\frac{u}{v}$, where $u = \\sum((y_{true}-y_{pred})^2)$ and $v = \\sum((y_{true}-\\mu_{y_{true}})^2)$, which in simpler terms is a measure of how much the predicted values vary from the true values compared to how much the true values vary from the mean of the true values.\n",
    "\n",
    "Lastly, the predicted values of each regressor were inspected visually by their residual plots, because it can be a bit hard to interpret $R^2$ by itself, and plotting the data displays how the regressors estimate.\n",
    "\n",
    "The reasen for choosing regression was to be able to predict numerical numbers and the choice of using these two specificially was to try something novel. One can imagine that predicting the amount of accidents on an hourly level in every borough could be a huge benefit e.g. to schedule manpower in the different police precints. The model made here is rather simple, but could easily be expandend upon with e.g. access to weather data. \n",
    "\n",
    "The results can be seen below all the code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "This subsection is most explained by inline comments. The $R^2$ values are right after each regressor code block and will also be summarized in the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grouping by the features we are interested in and getting their sizes\n",
    "simple_series = df.groupby([\"borough\", \"year\", \"month\", \"weekday\", \"hour\"]).size()\n",
    "\n",
    "# turn the series into a data frame\n",
    "simple_df = pd.DataFrame(simple_series)\n",
    "\n",
    "# save data frame to csv and load it agagin \n",
    "# (a workaround - I did not know have to directly \n",
    "# put the features that I grouped by into every row)\n",
    "simple_df.to_csv(\"test.csv\")\n",
    "new_simple_df = pd.read_csv(\"test.csv\")\n",
    "\n",
    "# label the columns again and the new \"count\" column created by size()\n",
    "new_simple_df.columns = [\"borough\", \"year\", \"month\", \"weekday\", \"hour\", \"count\"]\n",
    "\n",
    "# transform the categorial values into numerical values\n",
    "columns_to_transform = [\"borough\", \"year\", \"month\", \"weekday\", \"hour\"]\n",
    "new_simple_df_with_dummies = pd.get_dummies(new_simple_df, columns = columns_to_transform )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R-squared: 0.83\n"
     ]
    }
   ],
   "source": [
    "# random forest regressor without SelectFromModel feature selection\n",
    "\n",
    "X = new_simple_df_with_dummies.drop(\"count\", 1)\n",
    "y = new_simple_df[\"count\"]\n",
    "\n",
    "# split the data and target for train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "rfg = RandomForestRegressor(n_estimators=100)\n",
    "rfg = rfg.fit(X_train, y_train)\n",
    "\n",
    "scores = cross_val_score(rfg, X_test, y_test, cv=5)\n",
    "\n",
    "print(\"R-squared: {:.2f}\".format(sum(scores)/5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random forest regressor with SelectFromModel feature selection\n",
    "\n",
    "X = new_simple_df_with_dummies.drop(\"count\", 1)\n",
    "y = new_simple_df[\"count\"]\n",
    "\n",
    "# split the data and target for train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "# create random forest regressor with 100 estimators\n",
    "rfg_sfm = RandomForestRegressor(n_estimators=100)\n",
    "\n",
    "# create a SelectFromModel to choose the most important features from data\n",
    "sfm = SelectFromModel(rfg_sfm, threshold=0.001)\n",
    "sfm.fit(X, y)\n",
    "\n",
    "new_X = sfm.transform(X)\n",
    "X_train, X_test, y_train, y_test = train_test_split(new_X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "rfg_sfm.fit(X_train, y_train)\n",
    "\n",
    "scores = cross_val_score(rfg_sfm, X_test, y_test, cv=5)\n",
    "\n",
    "print(\"R-squared: {:.2f}\".format(sum(scores)/5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLPRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLP regressor\n",
    "\n",
    "X = new_simple_df_with_dummies.drop(\"count\", 1)\n",
    "y = new_simple_df[\"count\"]\n",
    "\n",
    "# split the data and target for train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# create a MLP regressor\n",
    "mlp_reg = MLPRegressor(random_state=42)\n",
    "\n",
    "# fit the regressor\n",
    "mlp_reg.fit(X_train, y_train)\n",
    "\n",
    "scores = cross_val_score(mlp_reg, X_test, y_test, cv=5)\n",
    "\n",
    "print(\"R-squared: {:.2f}\".format(sum(scores)/5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The residual plots of the regressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data for the scatter plots\n",
    "rfg_predicted = rfg_sfm.predict(X_test)\n",
    "mlp_predicted = mlp_reg.predict(X_test)\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "# Two subplots, unpack the axes array immediately\n",
    "f, (ax1, ax2) = plt.subplots(1, 2, sharey=True, figsize=(10,5))\n",
    "\n",
    "ax1.set_title('RandomForestRegressor')\n",
    "ax2.set_title('MLPRegressor')\n",
    "\n",
    "ax1.scatter(y_test, rfg_predicted)\n",
    "ax2.scatter(y_test, mlp_predicted)\n",
    "\n",
    "# numbers to prodcue an identity line\n",
    "t = np.arange(0., 78, 1)\n",
    "\n",
    "# red identity line\n",
    "ax1.plot(t, t, 'r--')\n",
    "ax2.plot(t, t, 'r--')\n",
    "\n",
    "# set axes limits\n",
    "ax1.set_xlim([-5, 75])\n",
    "ax1.set_ylim([-5, 75])\n",
    "ax2.set_xlim([-5, 75])\n",
    "\n",
    "# set axes labels by setting text on the figure\n",
    "f.text(0.5, 0.00, 'Actual', ha='center', va='center')\n",
    "f.text(0.00, 0.5, 'Predicted', ha='center', va='center', rotation='vertical')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "The $R^2$ yielded a score of $0.82$ for the `RandomForestRegressor` both with and without the use of the `SelectFromModel`. The results seems pretty good the this. One could argue that a lot of other variables have a huge influence on the number of accidents on an hourly basis besides the ones chosen in this case. However, it was also a bit surprising it did not improve the score of removing any of the features using the `SelectFromModel`.\n",
    "\n",
    "Th `MLPRegressor` yielded an $R^2$ score of $0.84$, thereby a tiny bit better than the `RandomForestRegressor`. From the residual plot it can acutally be seen visually, or a least it seems so, that `MLPRegressor` performed best. The points are located more densely around the red line, that indicates a perfect prediction, and also seems better guessing the higher values in general. However, it also evident that it sometimes predict a negative amount which can obviously never be the case. The `MLPRegressor` can be tweaked a lot, and it would be very interesting to work more on this, especially with more features."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
